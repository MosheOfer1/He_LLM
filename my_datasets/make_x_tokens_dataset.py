# import sys
# import os
# import torch

# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# from custom_datasets.create_datasets import read_file_to_string, read_file_lines

# # text_file_path = input("Enter text file path: ")
# text_file_path = "my_datasets/SVLM_Hebrew_Wikipedia_Corpus.txt"

# text_list = read_file_lines(text_file_path)

# tokens1 = tokenizer1.tokenize(text)
# with tokenizer2.as_target_tokenizer():
#     tokens2 = tokenizer2.tokenize(text)
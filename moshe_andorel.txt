Im working with: cuda
MyCustomModel.__init__ - uses: cuda
HelsinkiTranslator.__init__ - uses: cuda
Translator.__init__ - uses: cuda
FacebookLLM.__init__ - uses: cuda
Transformer.__init__ - uses: cuda
Transformer2.__init__ - uses: cuda
len(text) = 2047974
len(data) = 2, len(clean_data) = 1638379
len(data) = 2, len(clean_data) = 1572476
len(data) = 2, len(clean_data) = 409594
len(data) = 2, len(clean_data) = 395473


 epoch = 335957, total = 1679785, warmup = 167978 


CombinedTrainer.__init__ - uses: cuda
transformer.transformer1.encoder.rnn.weight_ih_l0
transformer.transformer1.encoder.rnn.weight_hh_l0
transformer.transformer1.encoder.rnn.bias_ih_l0
transformer.transformer1.encoder.rnn.bias_hh_l0
transformer.transformer1.encoder.rnn.weight_ih_l1
transformer.transformer1.encoder.rnn.weight_hh_l1
transformer.transformer1.encoder.rnn.bias_ih_l1
transformer.transformer1.encoder.rnn.bias_hh_l1
transformer.transformer1.decoder.rnn.weight_ih_l0
transformer.transformer1.decoder.rnn.weight_hh_l0
transformer.transformer1.decoder.rnn.bias_ih_l0
transformer.transformer1.decoder.rnn.bias_hh_l0
transformer.transformer1.decoder.rnn.weight_ih_l1
transformer.transformer1.decoder.rnn.weight_hh_l1
transformer.transformer1.decoder.rnn.bias_ih_l1
transformer.transformer1.decoder.rnn.bias_hh_l1
transformer.transformer1.decoder.fc.weight
transformer.transformer1.decoder.fc.bias
transformer.transformer2.layer1.weight
transformer.transformer2.layer1.bias
transformer.transformer2.layer2.weight
transformer.transformer2.layer2.bias
{'loss': 7.9968, 'grad_norm': 3.4932382106781006, 'learning_rate': 2.6398984801796245e-07, 'epoch': 0.0}
{'loss': 9.077, 'grad_norm': 4.331735610961914, 'learning_rate': 6.411182023293374e-07, 'epoch': 0.0}
{'loss': 7.8574, 'grad_norm': 41.37648010253906, 'learning_rate': 1.0182465566407122e-06, 'epoch': 0.0}
{'loss': 8.3778, 'grad_norm': 48.18242263793945, 'learning_rate': 1.3576620755209497e-06, 'epoch': 0.0}
{'loss': 7.8765, 'grad_norm': 9.05294132232666, 'learning_rate': 1.7347904298323244e-06, 'epoch': 0.0}
{'loss': 8.798, 'grad_norm': 5.372531890869141, 'learning_rate': 2.1119187841436996e-06, 'epoch': 0.0}
{'loss': 9.11, 'grad_norm': 11.8351469039917, 'learning_rate': 2.4890471384550743e-06, 'epoch': 0.0}
{'loss': 10.1223, 'grad_norm': 14.058122634887695, 'learning_rate': 2.8661754927664494e-06, 'epoch': 0.0}
{'loss': 10.103, 'grad_norm': 4.176265239715576, 'learning_rate': 3.2433038470778246e-06, 'epoch': 0.0}
{'loss': 10.5297, 'grad_norm': 10.068120002746582, 'learning_rate': 3.5827193659580615e-06, 'epoch': 0.0}
{'loss': 9.5607, 'grad_norm': 7.111655235290527, 'learning_rate': 3.959847720269437e-06, 'epoch': 0.0}
{'loss': 8.9401, 'grad_norm': 15.943257331848145, 'learning_rate': 4.336976074580812e-06, 'epoch': 0.0}
{'loss': 9.6407, 'grad_norm': 4.932598114013672, 'learning_rate': 4.7141044288921864e-06, 'epoch': 0.0}
{'loss': 10.9809, 'grad_norm': 1.4304713010787964, 'learning_rate': 5.091232783203561e-06, 'epoch': 0.0}
{'loss': 8.4835, 'grad_norm': 13.344992637634277, 'learning_rate': 5.468361137514937e-06, 'epoch': 0.0}
{'loss': 9.6875, 'grad_norm': 12.509531021118164, 'learning_rate': 5.8454894918263114e-06, 'epoch': 0.0}
{'loss': 8.1142, 'grad_norm': 5.003198623657227, 'learning_rate': 6.222617846137686e-06, 'epoch': 0.0}
{'loss': 10.3328, 'grad_norm': 6.003563404083252, 'learning_rate': 6.599746200449061e-06, 'epoch': 0.0}
{'loss': 9.5897, 'grad_norm': 198.53256225585938, 'learning_rate': 6.9768745547604356e-06, 'epoch': 0.0}
{'loss': 9.9283, 'grad_norm': 24.11725425720215, 'learning_rate': 7.354002909071811e-06, 'epoch': 0.0}
{'loss': 9.8499, 'grad_norm': 7.847563743591309, 'learning_rate': 7.731131263383185e-06, 'epoch': 0.0}
{'loss': 10.2133, 'grad_norm': 21.85379981994629, 'learning_rate': 8.108259617694561e-06, 'epoch': 0.0}
{'loss': 9.1859, 'grad_norm': 6.109764575958252, 'learning_rate': 8.485387972005936e-06, 'epoch': 0.0}
{'loss': 8.0526, 'grad_norm': 18.58763313293457, 'learning_rate': 8.86251632631731e-06, 'epoch': 0.0}
{'loss': 10.117, 'grad_norm': 43.3914794921875, 'learning_rate': 9.239644680628686e-06, 'epoch': 0.0}
{'loss': 10.1253, 'grad_norm': 21.122146606445312, 'learning_rate': 9.61677303494006e-06, 'epoch': 0.0}
{'loss': 9.9886, 'grad_norm': 11.213357925415039, 'learning_rate': 9.993901389251435e-06, 'epoch': 0.0}
{'loss': 8.4256, 'grad_norm': nan, 'learning_rate': 1.0333316908131672e-05, 'epoch': 0.0}
